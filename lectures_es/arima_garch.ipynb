{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ARMIA and GARCH\n",
        "\n",
        "Espen Sirnes  \n",
        "2025-04-24\n",
        "\n",
        "# Overview\n",
        "\n",
        "The raw, non‑stationary residuals $\\mathbf{u}$ are defined as:\n",
        "\n",
        "<span id=\"eq-residuals\">$$\n",
        "\\mathbf{u} = \\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta},\n",
        " \\qquad(1)$$</span>\n",
        "\n",
        "where $\\mathbf{X}$ is an $T \\times K$ array of $K$ covariates across $N$\n",
        "groups and $T$ time periods, $\\boldsymbol{\\beta}$ is a $K \\times 1$\n",
        "coefficient vector, and $\\mathbf{y}$ is the $T \\times 1$ dependent\n",
        "variable.\n",
        "\n",
        "The reasons for handling this problems are\n",
        "\n",
        "-   Serial correlation in $\\mathbf u$ can understate standard errors,\n",
        "    artificially inflating $t$–statistics.\n",
        "-   Conditional heteroscedasticity violates the constant‑variance\n",
        "    assumption.\n",
        "-   Converting $\\mathbf{u}$ into *i.i.d.* innovations permits valid\n",
        "    inference.\n",
        "\n",
        "The objective is therefore to transform $\\mathbf{u}$ — typically\n",
        "serially correlated and heteroscedastic — into uncorrelated,\n",
        "homoscedastic residuals $\\boldsymbol{\\varepsilon}/\\boldsymbol{\\sigma}$\n",
        "by applying\n",
        "\n",
        "1.  **ARIMA filtering** removes mean persistence and unit‑roots,\n",
        "    delivering the quasi‑innovations $\\boldsymbol{\\varepsilon}$.\n",
        "2.  **GARCH standardisation** scales out time‑varying conditional\n",
        "    variance, yielding the final innovations\n",
        "    $\\tilde{\\varepsilon}_t = \\varepsilon_t/\\sigma_t$.\n",
        "\n",
        "Formally, the ARIMA step (Box et al. 2015) is\n",
        "\n",
        "<span id=\"eq-ARIMA\">$$\n",
        "\\boldsymbol{\\varepsilon} = \\mathbf{MA}^{-1} \\cdot \\mathbf{AR} \\cdot \\boldsymbol{\\Delta}_d \\mathbf{u}\n",
        " \\qquad(2)$$</span>\n",
        "\n",
        "where $\\mathbf{u}$ is the $T \\times 1$ vector, differenced $d$ times by\n",
        "$\\boldsymbol{\\Delta}_d$. $\\mathbf{MA}$ and $\\mathbf{AR}$ are\n",
        "$T \\times T$ moving‑average and autoregressive matrices.\n",
        "\n",
        "Heteroscedasticity is subsequently corrected by the GARCH (Bollerslev\n",
        "1986; Engle 1982) normalization:\n",
        "\n",
        "<span id=\"eq-GARCH\">$$\n",
        "\\mathbf{\\sigma}^{2}\n",
        "= \\mathbf{GA}_{\\sigma}^{-1}\\,\\mathbf{G} + \\mathbf{GA}_{\\sigma}^{-1}\\,\\mathbf{AR}_{\\sigma}\\boldsymbol{\\varepsilon}^{2}\n",
        " \\qquad(3)$$</span>\n",
        "\n",
        "where $\\boldsymbol{\\varepsilon}^{2}$ and $\\boldsymbol{\\sigma}^{2}$ are\n",
        "$T \\times 1$ vectors. $\\mathbf{GA}_\\sigma$ and $\\mathbf{AR}_\\sigma$\n",
        "embed the GARCH and ARCH lag polynomials.\n",
        "\n",
        "The matrix $\\mathbf{G}$ is a column vector $T \\times 1$ with zeros,\n",
        "except the first term is the initial variance.\n",
        "\n",
        "Let us now look a bit closer on how these transformations are obtained.\n",
        "\n",
        "# The ARIMA process\n",
        "\n",
        "As noted, $\\mathbf{u}$ is the $T \\times 1$ matrix of residuals. It is\n",
        "linked to the stationary residuals $\\boldsymbol{\\varepsilon}$ via an\n",
        "ARIMA process:\n",
        "\n",
        "<span id=\"eq-ARIMA_sum0\">$$\n",
        "\\boldsymbol{\\Delta}_d \\mathbf{u} = \\sum_{j=1}^{p} \\rho_{j} \\mathbf{L}^{j}  \\boldsymbol{\\Delta}_d \\mathbf{u} +  \\boldsymbol{\\varepsilon} + \\sum_{j=1}^{q} \\lambda_{j} \\mathbf{L}^{j}  \\boldsymbol{\\varepsilon}\n",
        " \\qquad(4)$$</span>\n",
        "\n",
        "where $\\mathbf{L}$ is the lag operator matrix, defined as:\n",
        "\n",
        "<span id=\"eq-L\">$$\n",
        "\\mathbf{L} =\n",
        "\\begin{bmatrix}\n",
        "0 &        &        &        &        \\\\\n",
        "1 & 0      &        &        &        \\\\\n",
        "0 & 1      & 0      &        &        \\\\\n",
        "\\vdots & \\ddots & \\ddots & \\ddots &        \\\\\n",
        "0 & \\cdots & 0 & 1 & 0\n",
        "\\end{bmatrix}_{T \\times T}\n",
        " \\qquad(5)$$</span>\n",
        "\n",
        "You can verify yourself that premultiplying a column vector by\n",
        "$\\mathbf{L}$ shifts its elements down by one period, inserting a zero in\n",
        "the first position. In this way, $\\mathbf{L}$ effectively applies a\n",
        "one-period lag to the variable it is multiplied with.\n",
        "\n",
        "Although <a href=\"#eq-ARIMA_sum0\" class=\"quarto-xref\">Equation 4</a> may\n",
        "appear complex at first glance—suggesting that “everything depends on\n",
        "everything”—it simplifies neatly when expressed using lag polynomials:\n",
        "<span id=\"eq-ARIMA_sum\">$$\n",
        "\\left( \\mathbf{I} - \\sum_{j=1}^{p} \\rho_{j} \\mathbf{L}^{j} \\right) \\boldsymbol{\\Delta}_d \\mathbf{u} = \\left( \\mathbf{I} + \\sum_{j=1}^{q} \\lambda_{j} \\mathbf{L}^{j} \\right) \\boldsymbol{\\varepsilon}\n",
        " \\qquad(6)$$</span>\n",
        "\n",
        "By compacting the notation and defining the lag polynomials as matrices,\n",
        "the expression becomes even clearer and more concise. The moving-average\n",
        "(MA) component can then be written as:\n",
        "\n",
        "<span id=\"eq-MA\">$$\n",
        "\\mathbf{MA} = \\mathbf{I} + \\sum_{j=1}^{q} \\lambda_{j} \\mathbf{L}^{j},\n",
        " \\qquad(7)$$</span>\n",
        "\n",
        "and the corresponding autoregressive (AR) term is\n",
        "\n",
        "<span id=\"eq-AR\">$$\n",
        "\\mathbf{AR} = \\mathbf{I} - \\sum_{j=1}^{p} \\rho_{j} \\mathbf{L}^{j}.\n",
        " \\qquad(8)$$</span>\n",
        "\n",
        "The MA matrix typically look like this\n",
        "\n",
        "<span id=\"eq-MA_exp\">$$\n",
        "\\mathbf{MA} =\n",
        "\\begin{bmatrix}\n",
        "1      &        &        &        &        &        \\\\\n",
        "\\lambda_1 & 1      &        &        &        &        \\\\\n",
        "\\lambda_2 & \\lambda_1 & 1      &        &        &        \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\ddots &        &        \\\\\n",
        "\\lambda_q & \\cdots & \\cdots & \\lambda_1 & 1      &        \\\\\n",
        "0      & \\ddots & \\ddots & \\ddots & \\ddots & \\ddots\n",
        "\\end{bmatrix}\n",
        " \\qquad(9)$$</span>\n",
        "\n",
        "And the AR matrix typically like this <span id=\"eq-AR_exp\">$$\n",
        "\\mathbf{AR} =\n",
        "\\begin{bmatrix}\n",
        "1      &        &        &        &        &        \\\\\n",
        "-\\rho_1 & 1      &        &        &        &        \\\\\n",
        "-\\rho_2 & -\\rho_1 & 1      &        &        &        \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\ddots &        &        \\\\\n",
        "-\\rho_p & \\cdots & \\cdots & -\\rho_1 & 1      &        \\\\\n",
        "0      & \\ddots & \\ddots & \\ddots & \\ddots & \\ddots\n",
        "\\end{bmatrix}\n",
        " \\qquad(10)$$</span>\n",
        "\n",
        "Hence, we can write\n",
        "(<a href=\"#eq-ARIMA_sum\" class=\"quarto-xref\">Equation 6</a>) in matrix\n",
        "notation as:\n",
        "\n",
        "<span id=\"eq-ARIMA_matrix\">$$\n",
        "\\mathbf{AR} \\cdot \\boldsymbol{\\Delta}_d \\mathbf{u} = \\mathbf{MA} \\cdot \\boldsymbol{\\varepsilon}\n",
        " \\qquad(11)$$</span>\n",
        "\n",
        "Our goal is to isolate the independent random noise embedded in the\n",
        "differenced residuals $\\boldsymbol{\\Delta}_d\\mathbf{u}$. Recognising\n",
        "that this is a simple linear algebra problem, we can solve for the\n",
        "white-noise sequence $\\boldsymbol{\\varepsilon}$ by premultiplying both\n",
        "sides by the inverse of the moving-average operator $\\mathbf{MA}^{-1}$:\n",
        "\n",
        "<span id=\"eq-ARIMA_sol\">$$\n",
        "\\boldsymbol{\\varepsilon} = \\mathbf{MA}^{-1} \\cdot \\mathbf{AR} \\cdot \\boldsymbol{\\Delta}_d \\mathbf{u}\n",
        " \\qquad(12)$$</span>\n",
        "\n",
        "Hence, the white noise error terms $\\boldsymbol{\\varepsilon}$ can be\n",
        "obtained through standard linear algebra by inverting the moving average\n",
        "operator. However, in practice, explicitly computing the inverse is\n",
        "often computationally expensive, so a recursive filtering approach is\n",
        "typically preferred for efficiency.\n",
        "\n",
        "# The GARCH process\n",
        "\n",
        "The primary objective of the GARCH transformation is to correct for\n",
        "serially conditioned heteroskedasticity — that is, time-varying\n",
        "volatility in the error process. Conceptually, the procedure closely\n",
        "parallels the ARIMA approach, but with an inverted goal: whereas ARIMA\n",
        "seeks to eliminate temporal dependence in the residuals by extracting\n",
        "the *innovations*, GARCH focuses on modelling the *volatility* itself.\n",
        "\n",
        "In this framework, we estimate the serially correlated conditional\n",
        "standard deviation vector $\\boldsymbol{\\sigma}$, which is then used to\n",
        "standardise the stationary ARIMA residuals $\\boldsymbol{\\varepsilon}$,\n",
        "producing homoscedastic white noise via:\n",
        "\n",
        "<span id=\"eq-objective\">$$\n",
        "\\tilde{\\varepsilon}_t = \\varepsilon_t / \\sigma_t\n",
        " \\qquad(13)$$</span>\n",
        "\n",
        "This is conceptually the inverse of the ARIMA transformation, which\n",
        "seeks to *remove* serial correlation from the residuals. GARCH, by\n",
        "contrast, models that correlation in volatility.\n",
        "\n",
        "We begin with a general GARCH formulation. GARCH simply means that the\n",
        "current variance depends on the past variance and the previous squared\n",
        "residuals. This can be written as\n",
        "\n",
        "<span id=\"eq-GARCH_problem\">$$\n",
        "\\boldsymbol{\\sigma}^{2} = \\mathbf{G} + \\sum_{j = 1}^{m} \\psi_{j}\\mathbf{L}^{j}\\boldsymbol{\\varepsilon}^2 + \\sum_{j = 1}^{k} \\gamma_{j} \\mathbf{L}^{j} \\boldsymbol{\\sigma}^{2}\n",
        " \\qquad(14)$$</span>\n",
        "\n",
        "The matrix $\\mathbf{G}$ is a column vector $T \\times 1$ with zeros,\n",
        "except the first term is the initial variance. As with ARIMA, we can\n",
        "rearrange the GARCH equation into an explicit solvable form:\n",
        "\n",
        "<span id=\"eq-GARCH_solvable\">$$\n",
        "\\left( \\mathbf{I} - \\sum_{j = 1}^{k} \\gamma_{j} \\mathbf{L}^{j} \\right) \\boldsymbol{\\sigma}^{2} = \\mathbf{G} + \\sum_{j = 1}^{m} \\psi_{j} \\mathbf{L}^{j} h\\left( \\boldsymbol{\\varepsilon}, c \\right)\n",
        " \\qquad(15)$$</span>\n",
        "\n",
        "As with the ARIMA model, the problem becomes more transparent when we\n",
        "express the lag polynomials as matrix operators:\n",
        "\n",
        "<span id=\"eq-GAs\">$$\n",
        "\\mathbf{GA}_{\\sigma} = \\mathbf{I} - \\sum_{j = 1}^{k} \\gamma_{j} \\mathbf{L}^{j}\n",
        " \\qquad(16)$$</span>\n",
        "\n",
        "<span id=\"eq-ARs\">$$\n",
        "\\mathbf{AR}_{\\sigma} = \\sum_{j = 1}^{m} \\psi_{j} \\mathbf{L}^{j}\n",
        " \\qquad(17)$$</span>\n",
        "\n",
        "$\\mathbf{GA}_{\\sigma}$ and $\\mathbf{AR}_{\\sigma}$ have similar\n",
        "structures as <a href=\"#eq-MA_exp\" class=\"quarto-xref\">Equation 9</a>\n",
        "and <a href=\"#eq-AR_exp\" class=\"quarto-xref\">Equation 10</a>.\n",
        "\n",
        "Then, analogous to the ARIMA formulation, we can express the\n",
        "<a href=\"#eq-GARCH_solvable\" class=\"quarto-xref\">Equation 15</a> in\n",
        "operator form as:\n",
        "\n",
        "<span id=\"eq-GARCH_operator\">$$\n",
        "\\mathbf{GA}_{\\sigma} \\cdot \\boldsymbol{\\sigma}^{2} =  \\mathbf{G} + \\mathbf{AR}_{\\sigma} \\cdot \\boldsymbol{\\varepsilon}^2\n",
        " \\qquad(18)$$</span>\n",
        "\n",
        "As is standard with linear systems, we solve for $\\boldsymbol{\\sigma}^2$\n",
        "by premultiplying both sides by the inverse of $\\mathbf{GA}_{\\sigma}$:\n",
        "\n",
        "<span id=\"eq-GARCH_solution\">$$\n",
        "\\boldsymbol{\\sigma}^{2} = \\mathbf{GA}_{\\sigma}^{-1} \\left( \\mathbf{G} + \\mathbf{AR}_{\\sigma} \\cdot \\boldsymbol{\\varepsilon}^2 \\right)\n",
        " \\qquad(19)$$</span>\n",
        "\n",
        "This transformation deflates the residuals by their conditional standard\n",
        "deviation, yielding a series $\\varepsilon_t / \\sigma_t$ that is\n",
        "homoscedastic under the model.\n",
        "\n",
        "### Confusing terminology\n",
        "\n",
        "Following convention, the effects from $\\boldsymbol{\\varepsilon}^2$ are\n",
        "referred to as **autoregressive**, even though they represent the\n",
        "response to past innovations. Historically, the term GARCH was\n",
        "introduced by (Bollerslev 1986) to generalise the ARCH process by adding\n",
        "a persistent variance component.\n",
        "\n",
        "This naming choice may appear counterintuitive when compared to ARIMA\n",
        "models, where:\n",
        "\n",
        "-   **Autoregressive (AR)** components capture persistence over time.\n",
        "-   **Moving average (MA)** components capture transient innovation\n",
        "    shocks.\n",
        "\n",
        "In GARCH models, however, the roles are reversed:\n",
        "\n",
        "-   **ARCH terms** (lagged squared residuals) reflect immediate\n",
        "    volatility responses — akin to MA terms in ARIMA.\n",
        "-   **GARCH terms** (lagged variances) model long-run volatility\n",
        "    persistence — functionally closer to AR components.\n",
        "\n",
        "Thus, it might have been more consistent to label the ARCH component as\n",
        "the moving average part. One possible justification for the current\n",
        "convention is that the ARCH effect is the component inverted when\n",
        "solving the reduced-form variance equation. However, it is crucial to\n",
        "recognise that the dependent variable differs: ARIMA models solve for\n",
        "the signal $\\mathbf{y}$ (via $\\boldsymbol{\\varepsilon}$), while GARCH\n",
        "models solve directly for the conditional variance\n",
        "$\\boldsymbol{\\sigma}^{2}$.\n",
        "\n",
        "# Literature\n",
        "\n",
        "Bollerslev, Tim. 1986. “Generalized Autoregressive Conditional\n",
        "Heteroskedasticity.” *Journal of Econometrics* 31 (3): 307–27.\n",
        "\n",
        "Box, George EP, Gwilym M Jenkins, Gregory C Reinsel, and Greta M Ljung.\n",
        "2015. *Time Series Analysis: Forecasting and Control*. John Wiley &\n",
        "Sons.\n",
        "\n",
        "Engle, Robert F. 1982. “Autoregressive Conditional Heteroscedasticity\n",
        "with Estimates of the Variance of United Kingdom Inflation.”\n",
        "*Econometrica: Journal of the Econometric Society*, 987–1007."
      ],
      "id": "c3a40cf8-a975-45e8-8211-848264924709"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  }
}